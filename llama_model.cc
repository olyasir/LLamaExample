#include "llama_model.h"
#include<cpp/serve/model.h>

// LlamaModel::LlamaModel() {

    
//     //lib_ = tvm::runtime::Module::LoadFromFile("~/olya/mlc-llm/dist/libs/Meta-Llama-3.1-8B-Instruct-vulkan.so");
// }

// LlamaModel::LoadWeights(const std::string& weights_file) {

// }

// LlamaModel::Process(const std::vector<float>& input) {

// }
